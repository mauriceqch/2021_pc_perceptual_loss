{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from utils import pc_io\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from utils.perceptual_model import PerceptualModel, input_fn\n",
    "from utils.distance_grid import distance_grid_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and parameters\n",
    "train_glob = '../ModelNet40_200_pc512_oct3_4k/**/*.ply'\n",
    "\n",
    "dense_tensor_shape = np.array([1, 64, 64, 64])\n",
    "resolution = 64\n",
    "alpha = 0.75\n",
    "gamma = 2.0\n",
    "data_format = 'channels_first'\n",
    "# binary: binary occupancy map\n",
    "# tdf: truncated distance field\n",
    "# To reproduce results, two models should be trained: one for binary blocks and one for tdf blocks\n",
    "# To do so, comment/uncomment the corresponding lines below\n",
    "# Model for binary\n",
    "# data_mode = 'binary'\n",
    "# checkpoint_dir = 'data/model'\n",
    "# Model for tdf\n",
    "data_mode = 'tdf'\n",
    "checkpoint_dir = 'data/model_tdf'\n",
    "# tdf upper bound\n",
    "tdf_ub = 3\n",
    "# Training\n",
    "batch_size = 32\n",
    "validation_interval = 500\n",
    "early_stop_patience = validation_interval * 4\n",
    "validation_steps = 8\n",
    "summary_interval = 250\n",
    "max_steps = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "files = pc_io.get_files(train_glob)\n",
    "assert len(files) > 0\n",
    "points = pc_io.load_points(files)\n",
    "print(f'Loaded {len(files)} files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_cat = np.array([os.path.split(os.path.split(x)[0])[1] for x in files])\n",
    "points_train = points[files_cat == 'train']\n",
    "points_val = points[files_cat == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_mode == 'tdf':\n",
    "    points_train_tdf = [distance_grid_3d(x, dense_tensor_shape[1:], ub=tdf_ub).astype(np.float32) for x in tqdm(points_train)]\n",
    "    points_val_tdf = [distance_grid_3d(x, dense_tensor_shape[1:], ub=tdf_ub).astype(np.float32) for x in tqdm(points_val)]\n",
    "    points_train_tdf = [x[np.newaxis] for x in points_train_tdf]\n",
    "    points_val_tdf = [x[np.newaxis] for x in points_val_tdf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn_tdf(points, batch_size, dense_tensor_shape, data_format, repeat=True, shuffle=True, prefetch_size=1):\n",
    "    # Create input data pipeline.\n",
    "    with tf.device('/cpu:0'):\n",
    "        dataset = tf.data.Dataset.from_generator(lambda: iter(points), tf.float32, tf.TensorShape(dense_tensor_shape))\n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(len(points))\n",
    "        if repeat:\n",
    "            dataset = dataset.repeat()\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        dataset = dataset.prefetch(prefetch_size)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_mode == 'binary':\n",
    "    train_ds = input_fn(points_train, batch_size, dense_tensor_shape, data_format, repeat=True, shuffle=True)\n",
    "    val_ds = input_fn(points_val, batch_size, dense_tensor_shape, data_format, repeat=True, shuffle=True)\n",
    "elif data_mode == 'tdf':\n",
    "    train_ds = input_fn_tdf(points_train_tdf, batch_size, dense_tensor_shape, data_format, repeat=True, shuffle=True)\n",
    "    val_ds = input_fn_tdf(points_val_tdf, batch_size, dense_tensor_shape, data_format, repeat=True, shuffle=True)\n",
    "else:\n",
    "    raise RuntimeError(f'Unknown data mode {data_mode}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize input pipeline\n",
    "train_iterator = tf.data.make_one_shot_iterator(train_ds)\n",
    "val_iterator = tf.data.make_one_shot_iterator(val_ds)\n",
    "handle = tf.placeholder(tf.string, shape=[], name='handle')\n",
    "output_shapes = tf.data.get_output_shapes(train_ds)\n",
    "iterator = tf.data.Iterator.from_string_handle(handle, tf.data.get_output_types(train_ds))\n",
    "x = tf.placeholder_with_default(iterator.get_next(), output_shapes, name='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PerceptualModel()\n",
    "x_tilde = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init optimization\n",
    "if data_mode == 'binary':\n",
    "    from utils.focal_loss import focal_loss\n",
    "    train_loss = focal_loss(x, x_tilde, alpha=alpha, gamma=gamma)\n",
    "elif data_mode == 'tdf':\n",
    "    loss_mask = tf.maximum(tf.minimum(tf.cast(x < 1, tf.float32), alpha), 1 - alpha)\n",
    "    train_loss = tf.reduce_mean(tf.square(x - x_tilde) * loss_mask)\n",
    "else:\n",
    "    raise RuntimeError(f'Unknown data mode {data_mode}')\n",
    "\n",
    "step = tf.train.get_or_create_global_step()\n",
    "main_optimizer = tf.train.AdamOptimizer()\n",
    "train_op = main_optimizer.minimize(train_loss, global_step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.get_default_graph()\n",
    "model_name = model.name\n",
    "tensors = {\n",
    "    'x': x,\n",
    "    'x1': graph.get_tensor_by_name(f'{model_name}/conv3d_0/Relu:0'),\n",
    "    'x2': graph.get_tensor_by_name(f'{model_name}/conv3d_1/Relu:0'),\n",
    "    'y0': graph.get_tensor_by_name(f'{model_name}/conv3d_2/Relu:0'),\n",
    "    'y1': graph.get_tensor_by_name(f'{model_name}/conv3dt_0/Relu:0'),\n",
    "    'y2': graph.get_tensor_by_name(f'{model_name}/conv3dt_1/Relu:0'),\n",
    "    'x_tilde': graph.get_tensor_by_name(f'{model_name}/conv3dt_2/Sigmoid:0'),\n",
    "}\n",
    "for name in tensors:\n",
    "    tf.summary.histogram(name, tensors[name])\n",
    "tf.summary.scalar('train_loss', train_loss)\n",
    "merged_summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[n.name for n in tf.get_default_graph().as_graph_def().node]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "# Summary writers\n",
    "train_writer = tf.summary.FileWriter(os.path.join(checkpoint_dir, 'train'))\n",
    "val_writer = tf.summary.FileWriter(os.path.join(checkpoint_dir, 'val'))\n",
    "\n",
    "# Checkpoints\n",
    "saver = tf.train.Saver(save_relative_paths=True)\n",
    "checkpoint_path = os.path.join(checkpoint_dir, 'model.ckpt')\n",
    "# Init\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "print('Starting session')\n",
    "tf_config = tf.ConfigProto()\n",
    "tf_config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=tf_config) as sess:\n",
    "    print('Init session')\n",
    "    sess.run(init)\n",
    "\n",
    "    train_handle, test_handle = sess.run([train_iterator.string_handle(), val_iterator.string_handle()])\n",
    "\n",
    "    checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    if checkpoint is not None:\n",
    "        print(f'Restoring checkpoint {checkpoint}')\n",
    "        saver.restore(sess, checkpoint)\n",
    "    train_writer.add_graph(sess.graph)\n",
    "\n",
    "    step_val = sess.run(step)\n",
    "    first_step_val = step_val\n",
    "    pbar = tqdm(total=max_steps)\n",
    "    print(f'Starting training')\n",
    "    best_loss = 1e+32\n",
    "    best_loss_step = step_val\n",
    "    while step_val <= max_steps:\n",
    "        pbar.update(step_val - pbar.n)\n",
    "\n",
    "        # Validation\n",
    "        if step_val != first_step_val and step_val % validation_interval == 0:\n",
    "            print(f'{datetime.now().isoformat()} Executing validation')\n",
    "            losses = []\n",
    "            for i in trange(validation_steps):\n",
    "                summary, vloss = sess.run([merged_summary, train_loss], feed_dict={handle: test_handle})\n",
    "                losses.append(vloss)\n",
    "                val_writer.add_summary(summary, step_val + i)\n",
    "            loss = np.mean(losses)\n",
    "            print('')\n",
    "\n",
    "            # Early stopping\n",
    "            if (loss - best_loss) / best_loss < -1e-3:\n",
    "                print(f'Val loss {loss:.3E}@{step_val} lower than previous best {best_loss:.3E}@{best_loss_step}')\n",
    "                best_loss_step = step_val\n",
    "                best_loss = loss\n",
    "                save_path = saver.save(sess, checkpoint_path, global_step=step_val)\n",
    "                print(f'Model saved to {save_path}')\n",
    "            elif step_val - best_loss_step >= early_stop_patience:\n",
    "                print(f'Val loss {loss:.3E}@{step_val} higher than previous best {best_loss:.3E}@{best_loss_step}')\n",
    "                print(f'Early stopping')\n",
    "                break\n",
    "            else:\n",
    "                print(f'Val loss {loss:.3E}@{step_val} higher than previous best {best_loss:.3E}@{best_loss_step}')\n",
    "\n",
    "        # Training\n",
    "        get_summary = step_val % summary_interval == 0\n",
    "        sess_args = {'train_op': train_op, 'train_loss': train_loss}\n",
    "        if get_summary:\n",
    "            sess_args['merged_summary'] = merged_summary\n",
    "        sess_output = sess.run(sess_args, feed_dict={handle: train_handle})\n",
    "\n",
    "        step_val += 1\n",
    "        if get_summary:\n",
    "            train_writer.add_summary(sess_output['merged_summary'], step_val)\n",
    "\n",
    "        pbar.set_description(f\"loss: {sess_output['train_loss']:.3E}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(os.path.join(checkpoint_dir, 'done')).touch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
